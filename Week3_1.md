# Reinforcement learning from human feedback

## Introduction
1. RLHF
2. How to use LLMs as a reasoning engine and let it cause our own of routines to create an agent

## Aligning models with human values
1. Some LLM behaving badly beacuse LLM are trained on vast amounts of texts data from the Internet where such language appears frequently.
   - Toxic language
   - Agressive responses
   - Providing dangerous information
2. HHH: helpfulness, honesty, harmlessness
   - are a set of principles that guide developers in the responsible use of AI
3. Additional fine-tuning with human feedback helps to better align models with human preferences and to increase the HHH of the completions.
   This further training can also help to decrease the toxicity, often models responses and reduce the generation of incorrect information. 
