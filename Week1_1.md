# Week 1-1 (Introduction to LLMs and the generative AI project lifecycle)

The official lecture notes is on : https://community.deeplearning.ai/t/genai-with-llms-lecture-notes/361913

## Course Introduction 
1. Week 1
   - Transformer architecture
   - in-context learning
   - tuning parameters
2. Week 2
   - pre-trained model
   - instruction fine tuning
3. Week 3
   - align the output of language models with human values
   - Increase helpfullness, decrease potential harm and toxicity

## Introduction - Week 1
1. Transformer architecture, self-attention, multi-headed self-attention mechanism
2. Generative AI project lifecycle

## Generative AI & LLMs
1. Generative AI is a subset of traditional machine learning.
2. The following photos are a collection of foundation models, sometimes called 'base models'.
![image](https://github.com/FionaYuY/Generative-AI-with-Large-Language-Models_notes/blob/51907c27bf39acd139b3aa5cd88b6fcfff0420e9/screenshots%20of%20lecture%20slides/0008.jpg)
3. The more parameters a model has, the more memory, and the more sophisticated the tasks it can perform
4. Generative AI models are being created for 'multiple modalities'.
5. LLMs are able to take natuaral language or human written instructions and perform taskes much as a human would.
6. The text you pass to an LLm is 'prompt', and the space or memory that is available to the prompt is 'context window', and the output of the model is 'completion'. The act of using the model to generate text is 'inference'.
7. The 'completion' is comprised of the text contained in the original prompt, followed by the generated text.

## LLM use cases and tasks
